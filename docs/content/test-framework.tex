%----------------------------------------------------------------------------
\chapter{Test Framework}
%----------------------------------------------------------------------------

This chapter presents the design and implementation of the test framework that facilitates the automatic deployment of the sample application and running the measurements to determine the dependability metrics of the system.

%----------------------------------------------------------------------------
\section{Design}
%----------------------------------------------------------------------------

Consistency is essential when testing. In order to get meaningful results that can be later used to evaluate the implementation and to draw conclusions, a well defined framework is needed that sets clear boundaries on how the results are acquired. The quality of the testing and measurement greatly depends on the way these activities are executed, which means that a detailed design of the framework is vital.

%\begin{itemize}
%	\item why is a framework needed
%	\item test framework architecture
%	\item what is the test objective?
%	\item what to test? - dependability metrics! - which ones?, how?
%	\item need to monitor the application metrics, state --> need for monitoring system
%	
%	\item how does running tests affect the result
%\end{itemize}

%----------------------------------------------------------------------------
\subsection{Test objective} \label{test-design-objective}
%----------------------------------------------------------------------------

The first thing to specify when designing a test framework is the test objective. This highly affects the way the framework should be planned and created.

The goal of the framework is to measure the dependability metrics of the sample application to reveal weak spots and to discover opportunities to improve the dependability of the application. The results of the measurements can be used later as reference values when introducing various kinds of enhancements to the system to make the metrics better. The enhancements are described in Chapter \ref{enhancements}.

%----------------------------------------------------------------------------
\subsection{Requirements} \label{test-design-req}
%----------------------------------------------------------------------------

The design of the test framework was driven by several requirements to create an architecture that facilitates achieving the test objective defined in the previous section.

\paragraph{Configurable} In order to support various kinds of test scenarios, the framework should be highly configurable. This characteristic also makes the testing system more reusable and generic, so it is not closely tailored to the sample application described in this project.

\paragraph{Reproducible} The framework should function in a fairly deterministic way. This means that any two test executions with the same configuration should produce approximately the same results. As the applications that the framework is designed to evaluate are Kubernetes based, distributed systems, requiring complete deterministic behavior of the framework is not realistic. There are several things that can introduce a slight level of randomness to the application. For example the state of the network between the components or the way the fault injection logic inserts anomalies into the system (further details in section \ref{test-design-fault-injector}).

\paragraph{Automated} The usability of the framework greatly depends on how much manual interaction is needed for it produce results. Ideally, the framework should not require any human actions to be able to function after the configurations are correctly set. Automation enables the users of the framework to define multiple test scenarios with different configurations, collect them into a script and start all the measurements with only one command. Furthermore, if the framework is able to operate autonomously once it gets the right inputs, it can be integrated into existing pipelines (Continuous Integration, Continuous Delivery) to act as a quality gateway before an application is deployed into production.

%\begin{itemize}
%	\item Reproducible
%	\item Automated
%	\item Configurable
%\end{itemize}

%----------------------------------------------------------------------------
\subsection{Framework workflow} \label{test-design-workflow}
%----------------------------------------------------------------------------

%	\item Introduce abstract workflow \begin{enumerate}
%		\item Prepare infrastructure and application
%		\item Generate load
%		\item Inject faults
%		\item Collect metrics -  need to monitor the application metrics, state --> need for monitoring system
%		\item Store results
%	\end{enumerate}


Considering the test objective and the requirements described in the previous sections, the functional workflow of the testing framework can be seen in Figure \ref{fig:test_framework_workflow}.

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm, keepaspectratio]{figures/test_framework_workflow.png}
	\caption{Test framework workflow}
	\label{fig:test_framework_workflow}
\end{figure}

As per this design, the framework should first prepare the environment for testing. This consists of setting up the Kubernetes infrastructure that the application will run on and starting the deployed application in this environment. After the application is running, in order to trigger to trigger task executions, a subsystem generates and sends load to the application. To better understand how the system under test behaves, different kinds of faults and anomalies are introduced to the system while the executions and possibly the load generation are in progress. The framework constantly collects the exposed metrics from the application and the infrastructure to gain insights about how the system manages to handle the implanted anomalies. After the load generation and the task executions are done, the metrics get stored and evaluated.

%----------------------------------------------------------------------------
\subsection{Architecture}
%----------------------------------------------------------------------------

The architecture of the test framework that facilitates the previously defined workflow is displayed in Figure \ref{fig:test_framework_arch}.

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm, keepaspectratio]{figures/test_framework_arch.png}
	\caption{Test framework architecture}
	\label{fig:test_framework_arch}
\end{figure}

The framework consists of three main components: the load generator, the fault injector and the monitoring subsystem. Each is described in the following sections.


%\begin{itemize}
%	\item architecture DIAGRAM
%	\item describe what each component does
%\end{itemize}


%----------------------------------------------------------------------------
\subsection{Load Generator}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item send HTTP requests to the backend according to pattern (uniform distribution)
%\end{itemize}

The responsibility of the load generator component is to simulate real world workload on the sample application. This way the framework can better assess the actual dependability characteristics of the system.

As the sample application (more precisely the backend) accepts user inputs in the form of HTTP requests, the load generator component should be able to produce correctly built up HTTP requests and send them to the application. This will trigger task executions and multiple worker instance deployments in case the requests are sent more frequently than a single worker could execute them one by one without leaving any tasks waiting in the message queue.

%----------------------------------------------------------------------------
\subsubsection{Parameters}
%----------------------------------------------------------------------------

% input value
% wait time between requests - different distributions
% number of simulated users

There are several ways to customize the way that load generation happens and how much workload it causes to the system under test.

\paragraph{Task input value} Probably the most trivial way is to change the task input value in the requests. As described in section \ref{impl-backend-interfaces}, the \texttt{input} field in the request JSON determines which Fibonacci number is calculated by a worker component. This means the time a worker instance spends on executing a task can be manipulated by the input value.

\paragraph{Request frequency} Another method that the load generator can exploit to control the workload on the system is to wait different amounts of time between sending individual requests. This can also model how real users would use the application. The wait time between requests can be a constant value, but in fact, it can be calculated based upon any arbitrary logic. Generally, various kinds of distribution functions are used to determine the request frequency.

\paragraph{Parallelism} To increase the workload on the system, multiple requests can be sent by the load generator in parallel. Using this together with increasing the request frequency can greatly intensify the load on the application. Similarly to the request frequency, the amount of requests to send in parallel can be calculated using any logic.


%----------------------------------------------------------------------------
\subsection{Fault Injector} \label{test-design-fault-injector}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item inject faults on different levels (Pod, Network, CPU, Memory)
%	\item periodic fault injection
%\end{itemize}

The aim of the fault injector component is to introduce different kinds of failure anomalies to the application in order to trigger fault tolerant mechanisms in the system if they exist.

The amount of possible failure anomalies proportionally increases with the complexity of the system and is most likely to be close to practically countless variations. To achieve the best results while limiting the number of failure anomalies to test, only the most common failure scenarios should be taken into account. Considering also the probability of the failure anomalies and applying only those to the system can result in a fairly good approximation of real world failure scenarios.

The following sections attempt to collect some of the most common anomalies that can cause errors or even outages in cloud based systems.

%----------------------------------------------------------------------------
\subsubsection{General failure anomalies}
%----------------------------------------------------------------------------

\paragraph{Network anomalies} Network anomalies include latency, jitter, slow bandwith, network partition. All of these can affect the performance and dependability of an application.

\paragraph{CPU starvation} CPU starvation can occur if there are multiple software components deployed to the same physical machine which is the case most of the time when using the products of cloud provider companies to run an application. If a process on a machine executes a CPU heavy computation, it can reserve most of the CPU capacity of the machine, leaving other processes without enough processor time which can cause transient or even permanent application failures.

\paragraph{Memory starvation} Memory starvation is similar to CPU starvation, only instead of sharing a machine with CPU usage heavy processes, there are programs that take up most of the memory of the physical machine.

\paragraph{Disk anomalies} Disk anomalies are probably the most common failure anomalies as these type of devices are used in enormous amounts in every data center. This class of anomalies can frequently occur in several different forms, for example disk I/O latency, read/write errors or complete device failures.

%----------------------------------------------------------------------------
\subsubsection{Kubernetes failure anomalies}
%----------------------------------------------------------------------------

As the target platform is Kubernetes, the final list of applicable failure scenarios should include Kubernetes related anomalies as well. Some possible examples are listed below.

\paragraph{Pod failure} Pod failures can happen for example in case of configuration errors or application errors. Typically, when a pod fails, it gets restarted by a \texttt{ReplicaSet} that may solve transient errors.

\paragraph{Pod death} Pods can be terminated due to human operational faults but also when the Kubernetes control plane decides to reschedule a pod to a different physical machine.

\paragraph{Cloud provider anomalies} Although Kubernetes clusters can be set up on private machines as well, generally they are used as a service offered by a cloud provider company. These products are highly available and are managed by experts, however, occasional errors can possibly happen in these type of environments too.

%----------------------------------------------------------------------------
\subsection{Monitoring} \label{test-design-monitoring}
%----------------------------------------------------------------------------
%need to monitor the application metrics, state --> need for monitoring system
% which metrics to collect
% whicch dependability metrics to use
% timeseries metrics
% vizualisation capabilities

The framework should be able to collect metrics about the infrastructure and the sample application. This requires that the framework should have a separate monitoring subsystem.

As the value of different metrics changes over time while the system executes task and while various failure anomalies are injected, the monitoring system should be able to handle time series data format effectively.

In order to increase the human usability of the framework, it is required that the monitoring system provides a visual interface where its users can track the status of the system under test during execution and to evaluate results.

There are a lot of metrics that the monitoring system can collect. Apart from the application specific custom metric like \texttt{worker\_busy\_thread} introduced in section \ref{impl-worker-custom-metrics}, Kubernetes itself exposes a vast amount of metrics in several categories. As the focus of the thesis is on measuring the dependability metrics, only the values related to this should be collected and used. Section \ref{background-dep-metrics-mean-values} and \ref{background-dep-metrics-prob-funcs} describes the possible dependability metrics that can be measured. The monitoring system targets collecting measurements for the following dependability metrics:

\begin{itemize}
	\item Mean Up Time
	\item Mean Down Time
	\item Mean Time Between Failures
	\item Availability
\end{itemize}

These metrics were chosen because they acceptably describe the dependability characteristics of a cloud based system that is designed to function continuously. 

%----------------------------------------------------------------------------
\section{Implementation}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item Test environment locally or in cloud
%	\item how does running tests affect the 
%\end{itemize}

This section describes the implementation of the test framework and presents the decisions behind them.

Before going into the details of each subsystem in the framework, the environment of the test framework should be specified. The question is, where to run the framework? Should it be deployed next to the system under test to minimize communication overhead or should it run independently from the application?

Operating the test framework in the same environment as the tested application seems obvious at first, however this would bring in several issues that should be taken into account. Deploying the the framework and the system under the test would create a tightly coupled structure, whereas in distributed systems, loosely-coupled units are preferred. Co-deployment would also lead to the possibility that operating the framework could have a performance impact on the tested application itself, causing altered measurement results. Any infrastructural error (for example physical machine crash) -- either provoked by transient failures or inserted by the fault injector -- would affect both the application under test (which is accepted and should be handled by the application) and the framework itself. This is not passable as it would lead to framework malfunctions.

The conclusion is to deploy the test framework and the application to evaluate separately. As it is practical for some of the tools used in the framework to run in the same Kubernetes cluster as the application, separation is implemented with exploiting the namespacing capabilities of Kubernetes.

%----------------------------------------------------------------------------
\subsection{Kubedepend}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item what it is used for?
%	\item facilitates automatic load generation, fault injection, collecting results
%	\item describe its workflow
%	\item purpose of waiting for stable system state
%	\item prometheus metrics
%	\item parameters (click)
%\end{itemize}

Section \ref{test-design-req} emphasized the importance that the framework should be configurable, give reproducible results and be automated. To satisfy these requirements, a command line program in Python was created to enforce them as well as to implement the major part of the test framework workflow proposed in Section \ref{test-design-workflow}. This tool is named Kubedepend.

%----------------------------------------------------------------------------
\subsubsection{Configuration}
%----------------------------------------------------------------------------

Kubedepend supports easy configuration through exposing command line parameters that can be used to customize some behaviors of the tool.

This functionality is implemented with the help of the Click Python package \cite{Click}. Click enables creating clean command line interfaces in a composable way with as little code as necessary.

For instance, the user can specify the duration of the load generation, the types of failures anomalies to inject into the system and the number of tests to execute in sequence with the same configuration. The main configuration options are presented in later sections describing the implementation of the test framework workflow steps.

An example on how to define a command line parameter for the entrypoint of the Python program can be seen below.

\vspace{0.5cm}
\begin{minipage}{\linewidth}
	\begin{lstlisting}[language=python, caption={Define a parameter to control saving test results (simplified extract)}, label={lst:click-option}]
	@click.command()
	@click.option('--nosave', is_flag=True)
	def main(nosave):
		...
	
		click.echo('nosave=' + str(nosave))
		
		...
	\end{lstlisting}
\end{minipage}

%----------------------------------------------------------------------------
\subsubsection{Reproducibility}
%----------------------------------------------------------------------------

Kubedepend supports reproducible results in sense that when it is started with the same inputs, it executes the same logic to test and evaluate the system. 

%----------------------------------------------------------------------------
\subsubsection{Automation}
%----------------------------------------------------------------------------

After providing it with the right inputs, Kubedepend facilitates automatic load generation, fault injection and metric collection with the help of other third party tools later discussed in this chapter. When it is started, it is assumed that the Kubernetes infrastructure and the application are already ready and deployed. This means that it practically manages all the steps in Figure \ref{fig:test_framework_workflow} except the first one -- Prepare infrastructure and application. This step is executed by another mechanism that is later discussed (see Section \ref{cicd}).

%----------------------------------------------------------------------------
\subsection{Load generation}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item locust
%	\item usermodel
%	\item parameterization
%\end{itemize}

Kubedepend uses Locust \cite{Locust} to generate load for the sample application (more specifically for the backend component) to trigger executions. Locust is a scriptable and scalable performance testing tool that enables defining different user behaviors in regular Python code. Locust can be used as an independent tool with command line or graphical interface, but it is also possible to embed it into existing Python programs as a library. Kubedepend utilizes the functionality of Locust in the last mentioned way.

%----------------------------------------------------------------------------
\subsubsection{User Model}
%----------------------------------------------------------------------------

In order to generate load with Locust, a user model is required, that represents the behavior of one user. This model can specify the actions Locust will make against the backend \eg what request to send, how much time to wait between submitting task inputs.

\vspace{0.5cm}
\begin{minipage}{\linewidth}
	\begin{lstlisting}[language=python, caption={Locust user model}, label={lst:locust-user}]
	from locust import HttpUser, task, constant_pacing
	import constants as c # constants used in the program
	
	class User(HttpUser):
		# wait for an adaptive amount of time that ensures the task runs (at most) once every X seconds
		wait_time = constant_pacing(5)
		# the URL of the backend component
		host = c.BACKEND_HOST
		
		@task
		def submit_job_task(self):
			headers = {
			'Content-Type': 'application/json',
			'Accept': 'application/json'
			}
			data = {
			'input': 48
			}
			# make request
			self.client.post('/api/v1/jobs', json=data, headers=headers)
	\end{lstlisting}
\end{minipage}

The user model is a subclass of \texttt{HttpUser} which is the most commonly used model because it has a \texttt{client} attribute that can be used to make HTTP requests.

The \texttt{User} class has one custom method (\texttt{submit\_job\_task()}) that defines a Locust task. When the load generation is started, an instance of the \texttt{User} class will be created for each simulated user and while these run, they execute their Locust tasks which can be specified \eg by applying the \texttt{@task} decorator to member methods. The \texttt{submit\_job\_task()} creates a JSON object specifying the input for a new execution, then sends the request to the task input endpoint of the backend (see Section \ref{impl-backend-interfaces}).

The \texttt{wait\_time} method makes it possible to introduce delays after each Locust task execution. In the above example, it sets how many time to wait between sending inputs for new executions to the backend. The built-in \texttt{constant\_pacing} method is used for an adaptive time that ensures the requests are sent once every 5 seconds.

The \texttt{host} attribute stores the base URL of the backend service.

% TODO is the implementation of load generation (the sequence of locust command) needed here?

%----------------------------------------------------------------------------
\subsubsection{Parameters}
%----------------------------------------------------------------------------

Kubedepend exposes several command line options to configure the load generation mechanism:

\begin{itemize}
	\item \texttt{load-duration} - Specifies the duration of the load generation in a single measurement in seconds (further details about the measurement in Section \ref{test-impl-measurement}). 
	\item \texttt{locust-user-count} - Specifies the total number of Locust users to start in a single measurement
	\item \texttt{locust-spawn-rate} - Specifies the number of Locust users to start per second in a single measurement. Once the total number of users defined by \texttt{locust-user-count} is reached, no more simulated users are started. 
\end{itemize}


%----------------------------------------------------------------------------
\subsection{Fault Injection}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item deploy Chaos Mesh
%	\item define chaos experiments according to fault profiles (show one as example)
%	\item describe the usage of helm to deploy faults
%\end{itemize}

Fault injection is implemented by using Chaos Mesh (see details in Section \ref{background-chaos-mesh}).

In order to be able to inject failure anomalies, the core components of Chaos Mesh (Chaos Controller Manager, Chaos Daemon, Chaos Dashboard) must be deployed to the Kubernetes cluster prior to applying chaos experiments. It can be achieved using the official helm chart of Chaos Mesh \cite{ChaosMeshChart}. Kubedepend assumes that Chaos Mesh is already deployed on the Kubernetes cluster.

Kubedepend supports easy management of the chaos objects that represent failure anomalies. To avoid the need of individually creating, applying and deleting each chaos experiment Custom Resource Definition, these objects are collected into a custom helm chart called \texttt{kubedepend-chaos}. This also facilitates the easy configuration of chaos experiments through the use of helm templates and a values file.

%----------------------------------------------------------------------------
\subsubsection{Fault Profiles}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item list fault profiles and justification
%	\item parameterization
%\end{itemize}

Kubedepend defines fault profiles as a way of organizing chaos experiments into scenarios. Each fault profile attempts to describe a set of possible errors in a system that can arise. Applying these fault profiles and measuring the dependability metrics of the application can provide valuable insights about how the system would react to real world incidents.

The results produced by inserting the fault profiles can act as reference values when comparing different versions of the application under test. This notion is also used when different kind of enhancements are introduced to the system (see Chapter \ref{enhancements}).

The framework allows defining multiple chaos experiments inside one fault profile, however at this stage the fault profiles typically contain one experiment. 

It is also possible to set a strength level for each chaos experiment inside a fault profile. For example, when setting up a profile that uses a network delay chaos, the different strength levels control how much delay should be applied for each network packet. The exact values used for specifying the strength levels are arbitrary but can be modified easily if needed.

Currently the following fault profiles are defined:

\paragraph{IO} The IO fault profile introduces delays to disk read and write operations for a specific period of time. The current implementation limits the profile to only affect the database because IO operations are practically only critical for the database component.

\paragraph{Network Delay} The Network Delay fault profile causes latency in the network connections of the targeted Pods for a specific period of time.

\paragraph{Network Partition} The Network Partition fault profile separates Pods into several subnets by blocking communication between them for a specific period of time.

\paragraph{Pod Failure} The Pod Failure fault profile makes the targeted Pods unavailable for a specific period of time.

\paragraph{Pod Kill} The Pod Kill fault profile kills the targeted Pods triggering a restart if a ReplicaSet or similar mechanism is configured for the given Pod.

\paragraph{Stress CPU} The Stress CPU fault profile starts a specified number of worker processes that occupy a given percentage of CPU capacity in targeted Pods for a specified period of time.

\paragraph{Stress Memory} The Stress Memory fault profile starts a specified number of worker processes that that continuously allocate, read and write memory in the targeted Pods for a specified period of time.

\paragraph{None} The None fault profile does not apply any chaos experiments. It can be used to measure the failure free dependability characteristics of the system under test.

\paragraph{Custom} The Custom fault profile demonstrates the possibility of defining multiple chaos experiments inside a single fault profile. It consists of a Pod Failure and ab IO chaos experiments.

As an example, the chaos experiment in Pod Kill failure profile is defined the following way:

\vspace{0.5cm}
\begin{minipage}{\linewidth}
	\begin{lstlisting}[caption={Pod Kill chaos}, label={lst:pod-kill-chaos}]
	{{- if .Values.podKillChaos.enabled -}}
	apiVersion: chaos-mesh.org/v1alpha1
	kind: Schedule
	metadata:
		name: schedule-pod-kill-worker
	spec:
		schedule: '*/3 * * * *'
		historyLimit: 5
		concurrencyPolicy: 'Forbid'
		type: 'PodChaos'
		podChaos:
			action: pod-kill
			mode: fixed-percent
			{{- if eq .Values.podKillChaos.strength "low" }}
			value: {{ .Values.mode.fixPercent.low | quote }}
			{{- else if eq .Values.podKillChaos.strength "high" }}
			value: {{ .Values.mode.fixPercent.high | quote }}
			{{- else }}
			value: {{ .Values.mode.fixPercent.medium | quote }}
			{{- end }}
			selector:
				namespaces:
				- kubedepend
		duration: "60s"
	{{- end }}
	\end{lstlisting}
\end{minipage}

The pod kill chaos is defined under the \texttt{podChaos} key, the role of the \texttt{Schedule} object is described in Section \ref{test-impl-measurement}. The \texttt{mode: fixed-percent} configuration means that when the chaos experiment is active, it will randomly select a fixed percentage of eligible Pods (defined by the \texttt{selector}) to delete. The example uses helm template variables to set the value of the actual percentage based on which strength level is selected in the values file.

%----------------------------------------------------------------------------
\subsubsection{Parameters}
%----------------------------------------------------------------------------

Kubedepend exposes one command line option to configure the fault injection mechanism:

\begin{itemize}
	\item \texttt{fault-profile} - Specifies the name of the fault profile to apply to the system during fault injection.
\end{itemize}

%----------------------------------------------------------------------------
\subsection{Monitoring} \label{test-impl-monitoring}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item Deploy tools \begin{itemize}
%		\item Prometheus
%		\item Metrics server
%		\item Prometheus Adapter
%		\item Promethues Blackbox Exporter
%		\item Grafana
%	\end{itemize}
%	
%	
%\end{itemize}

As per the design description of the test framework discussed in Section \ref{test-design-monitoring}, the framework should provide a monitoring subsystem that is capable of collecting metrics related to the dependability of the application under test.

The monitoring system consists of several third-party tools in order to provide with the required functionality.

\paragraph{Prometheus} Prometheus is used to collect and store the metrics from the application components and the underlying Kubernetes infrastructure they run on.

\paragraph{Metrics Server} Metrics Server is a cluster-wide source of container metrics for Kubernetes built-in autoscaling pipelines and is needed to be able to user Prometheus Adapter.

\paragraph{Prometheus Adapter} The Prometheus Adapter enables Kubernetes to scale workloads (\eg Deployments) based upon custom metrics that are pulled from external metric provider like Prometheus.

\paragraph{Prometheus Blackbox Exporter} Prometheus Blackbox Exporter allows blackbox probing of endpoints over various protocols, for instance HTTP.

\paragraph{Grafana} Grafana is used by the framework to display the collected metrics and results on human-friendly dashboards.

The tools mentioned above are deployed using official helm charts \cite{PrometheusChart} \cite{PrometheusAdapterChart} \cite{BlackboxExporterChart} \cite{GrafanaChart} except the Metrics Server which is installed with plain Kubernetes definitions. 

%----------------------------------------------------------------------------
\subsubsection{Metric collection}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item prometheus annotations, scrape config
%\end{itemize}

With the default settings Prometheus already collects vast amounts of metrics from the Kubernetes layer. However, to collect metrics from the application components, further configuration is needed. To gather metrics from the worker instances, the following modifications are necessary.

\paragraph{Pod annotations} The worker Pods should be annotated with the following annotations to enable metric collection:

\vspace{0.5cm}
\begin{minipage}{\linewidth}
	\begin{lstlisting}[caption={Worker annotation to enable monitoring}, label={lst:worker-annotation-monitoring}]
	prometheus.io/scrape: "true"
	prometheus.io/path: "/actuator/prometheus"
	prometheus.io/port: "5000"
	\end{lstlisting}
\end{minipage}

\begin{itemize}
	\item \texttt{prometheus.io/scrape: "true"} Tells Prometheus to scrape the given Pod.
	\item \texttt{prometheus.io/path: "/actuator/prometheus"} Specifies the path where the metrics of the worker components are exposes in Prometheus compatible format.
	\item \texttt{prometheus.io/port: "5000"} Specifies which port to use to access the metrics.
\end{itemize}

\paragraph{Scrape job configuration} The scrape job configuration among other things specifies which targets to collects metrics from. To collect metrics from the worker Pods, the following settings are used (extract):

\vspace{0.5cm}
\begin{minipage}{\linewidth}
	\begin{lstlisting}[caption={Prometheus scrape configuration for worker pods}, label={lst:worker-pods-scrape-job}]
	job_name: worker-pods
	kubernetes_sd_configs:
	- role: pod
	  namespaces:
	    names:
	    - 'kubedepend'
	  selectors:
	  - role: pod
	    label: app=kubedepend-worker-app
	
	relabel_configs:
	- source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
	  action: keep
	  regex: true
	- source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
	  action: replace
	  target_label: __metrics_path__
	  regex: (.+)
	- source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
	  action: replace
	  regex: ([^:]+)(?::\d+)?;(\d+)
	  replacement: $1:$2
	  target_label: __address__
	\end{lstlisting}
\end{minipage}

The \texttt{kubernetes\_sd\_configs} object sets up Kubernetes Service Discovery which allows retrieving up to date scrape targets from the Kubernetes REST API. Here, the \texttt{pod} role is used, that specifies that the scrape job only looks for Pod objects. The targets are further filtered to be in the \texttt{kubedepend} namespace and have the label \texttt{app=kubedepend-worker-app}.

Relabeling is a powerful tool to dynamically configure the behavior of scraping. In the above three rules, Prometheus only keeps those targets, which posses the \texttt{prometheus.io/scrape: "true"} annotation. Then the metrics path -- which tells Prometheus on which path to access the exposed metrics -- is overwritten with value of \texttt{prometheus.io/path} annotation. Lastly, the address of the Pod is extended with the port number specified by the value of \texttt{prometheus.io/port} annotation.

%----------------------------------------------------------------------------
\subsubsection{Composite Metric Definitions}
%----------------------------------------------------------------------------
%	\item dependability metrics in PromQL
%	\item needed worker ratio
%	\item justify the 15 multiplier - it is the prometheus scrape interval

Throughout the project, several metrics are used that are not available as built-in values, rather composed from multiple basic metrics. These are the dependability metrics measured by the test framework and the custom metric used by the Horizontal Pod Autoscaler managing the replication of worker components.

 Section \ref{test-design-monitoring} described which dependability metrics are relevant in the framework to characterize the system. However, these metrics should be mapped to actual Prometheus queries in order to be able to use them. The following queries are built up from PromQL functions introduced in Section \ref{background-promql}.
 
%  TODO helper figures

\emph{Note:} In the implementation, the \texttt{probe\_success} metrics are used with the label matcher \texttt{instance=~".*kubedepend-backend.*"} but it was left out from here for the sake of readibility.

\emph{Note:} \texttt{<range\_length>}

\emph{Note:} multiplier 15
 
 \paragraph{Mean Up Time} asdfa
 
 \vspace{0.5cm}
 \begin{minipage}{\linewidth}
 	\begin{lstlisting}[caption={Mean Up Time defined in PromQL}, label={lst:promql-mut}]
 	15
 	* 
 	sum_over_time(
 		probe_success[<range_length>s]
 	)
 	/ 
 	floor(
 		(changes(
 			probe_success[<range_length>s]
 		)
 		+ 1
 		+ probe_success offset <range_length>s
 		)
 		/ 2
 	)
 	\end{lstlisting}
 \end{minipage}
 
 \paragraph{Mean Down Time} asfaf
 
 \vspace{0.5cm}
 \begin{minipage}{\linewidth}
 	\begin{lstlisting}[caption={Mean Up Time defined in PromQL}, label={lst:promql-mut}]
 	15 
 	* 
 	(count_over_time(
 		probe_success[<range_length>s])
 		- sum_over_time(
 			probe_success[<range_length>s]
 		)
 	)
 	/ 
 	floor(
 		(changes(
 			probe_success[<range_length>s]
 		)
 		+ 2
 		- probe_success offset <range_length>s
 		)
 		/ 2
 	)
 	\end{lstlisting}
 \end{minipage}
 
 \paragraph{Mean Time Between Failures} Is calculated by the sum of the above two metrics.
 
 \paragraph{Availability} afasdf
 
 \vspace{0.5cm}
 \begin{minipage}{\linewidth}
 	\begin{lstlisting}[caption={Availability defined in PromQL}, label={lst:promql-availability}]
 	avg_over_time(probe_success[<range_length>s])
 	\end{lstlisting}
 \end{minipage}

%----------------------------------------------------------------------------
\subsubsection{Parameters}
%----------------------------------------------------------------------------

Kubedepend does not expose any command line options to configure the monitoring as this subsystem is not part of it.

%----------------------------------------------------------------------------
\subsection{Measurement} \label{test-impl-measurement}
%----------------------------------------------------------------------------

% \item measurement sequence - what is the purpose of this
% the role of scheduled chaos experiments

%----------------------------------------------------------------------------
\subsubsection{Parameters}
%----------------------------------------------------------------------------

Kubedepend exposes two command line option to configure the measurement mechanism:

\begin{itemize}
	\item \texttt{nosave} - If provided, tells Kubedepend not to save the test configuration and the results.
	\item \texttt{measurement-count} - Specifies the number of measurements ot execute during a measurement sequence.
\end{itemize}

%----------------------------------------------------------------------------
\subsection{Kubedepend workflow}
%----------------------------------------------------------------------------

% TODO diagram about how Kubedepent connects to the different parts in the framework and the application

% wait for stable state -- why it is needed?


This section describes the implementation of the designed workflow (see Section \ref{test-design-workflow}) with the components presented in the previous sections.

%----------------------------------------------------------------------------
\subsection{CI/CD Pipeline} \label{cicd}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item CF template
%	\item Steps and what do they do
%	\item image about jenkins pipeline steps
%	\item parameters - passed to kubedepend
%\end{itemize}