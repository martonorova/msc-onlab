%----------------------------------------------------------------------------
\chapter{Test Framework}
%----------------------------------------------------------------------------

This chapter presents the design and implementation of the test framework that facilitates the automatic deployment of the sample application and running the measurements to determine the dependability metrics of the system.

%----------------------------------------------------------------------------
\section{Design}
%----------------------------------------------------------------------------

Consistency is essential when testing. In order to get meaningful results that can be later used to evaluate the implementation and to draw conclusions, a well defined framework is needed that sets clear boundaries on how the results are acquired. The quality of the testing and measurement greatly depends on the way these activities are executed, which means that a detailed design of the framework is vital.

%\begin{itemize}
%	\item why is a framework needed
%	\item test framework architecture
%	\item what is the test objective?
%	\item what to test? - dependability metrics! - which ones?, how?
%	\item need to monitor the application metrics, state --> need for monitoring system
%	
%	\item how does running tests affect the result
%\end{itemize}

%----------------------------------------------------------------------------
\subsection{Test objective}
%----------------------------------------------------------------------------

The first thing to specify when designing a test framework is the test objective. This highly affects the way the framework should be planned and created.

The goal of the framework is to measure the dependability metrics of the sample application to reveal weak spots and to discover opportunities to improve the dependability of the application.

%----------------------------------------------------------------------------
\subsection{Requirements} \label{test-design-req}
%----------------------------------------------------------------------------

The design of the test framework was driven by several requirements to create an architecture that facilitates achieving the test objective defined in the previous section.

\paragraph{Configurable} In order to support various kinds of test scenarios, the framework should be highly configurable. This characteristic also makes the testing system more reusable and generic, so it is not closely tailored to the sample application described in this project.

\paragraph{Reproducible} The framework should function in a fairly deterministic way. This means that any two test executions with the same configuration should produce approximately the same results. As the applications that the framework is designed to evaluate are Kubernetes based, distributed systems, requiring complete deterministic behavior of the framework is not realistic. There are several things that can introduce a slight level of randomness to the application. For example the state of the network between the components or the way the fault injection logic inserts anomalies into the system (further details in section \ref{test-design-fault-injector}).

\paragraph{Automated} The usability of the framework greatly depends on how much manual interaction is needed for it produce results. Ideally, the framework should not require any human actions to be able to function after the configurations are correctly set. Automation enables the users of the framework to define multiple test scenarios with different configurations, collect them into a script and start all the measurements with only one command. Furthermore, if the framework is able to operate autonomously once it gets the right inputs, it can be integrated into existing pipelines (Continuous Integration, Continuous Delivery) to act as a quality gateway before an application is deployed into production.

%\begin{itemize}
%	\item Reproducible
%	\item Automated
%	\item Configurable
%\end{itemize}

%----------------------------------------------------------------------------
\subsection{Framework workflow} \label{test-design-workflow}
%----------------------------------------------------------------------------

%	\item Introduce abstract workflow \begin{enumerate}
%		\item Prepare infrastructure and application
%		\item Generate load
%		\item Inject faults
%		\item Collect metrics -  need to monitor the application metrics, state --> need for monitoring system
%		\item Store results
%	\end{enumerate}


Considering the test objective and the requirements described in the previous sections, the functional workflow of the testing framework can be seen in Figure \ref{fig:test_framework_workflow}.

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm, keepaspectratio]{figures/test_framework_workflow.png}
	\caption{Test framework workflow}
	\label{fig:test_framework_workflow}
\end{figure}

As per this design, the framework should first prepare the environment for testing. This consists of setting up the Kubernetes infrastructure that the application will run on and starting the deployed application in this environment. After the application is running, in order to trigger to trigger task executions, a subsystem generates and sends load to the application. To better understand how the system under test behaves, different kinds of faults and anomalies are introduced to the system while the executions and possibly the load generation are in progress. The framework constantly collects the exposed metrics from the application and the infrastructure to gain insights about how the system manages to handle the implanted anomalies. After the load generation and the task executions are done, the metrics get stored and evaluated.

%----------------------------------------------------------------------------
\subsection{Architecture}
%----------------------------------------------------------------------------

The architecture of the test framework that facilitates the previously defined workflow is displayed in Figure \ref{fig:test_framework_arch}.

\begin{figure}[h]
	\centering
	\includegraphics[width=140mm, keepaspectratio]{figures/test_framework_arch.png}
	\caption{Test framework architecture}
	\label{fig:test_framework_arch}
\end{figure}

The framework consists of three main components: the load generator, the fault injector and the monitoring subsystem. Each is described in the following sections.


%\begin{itemize}
%	\item architecture DIAGRAM
%	\item describe what each component does
%\end{itemize}


%----------------------------------------------------------------------------
\subsection{Load Generator}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item send HTTP requests to the backend according to pattern (uniform distribution)
%\end{itemize}

The responsibility of the load generator component is to simulate real world workload on the sample application. This way the framework can better assess the actual dependability characteristics of the system.

As the sample application (more precisely the backend) accepts user inputs in the form of HTTP requests, the load generator component should be able to produce correctly built up HTTP requests and send them to the application. This will trigger task executions and multiple worker instance deployments in case the requests are sent more frequently than a single worker could execute them one by one without leaving any tasks waiting in the message queue.

%----------------------------------------------------------------------------
\subsubsection{Parameters}
%----------------------------------------------------------------------------

% input value
% wait time between requests - different distributions
% number of simulated users

There are several ways to customize the way that load generation happens and how much workload it causes to the system under test.

\paragraph{Task input value} Probably the most trivial way is to change the task input value in the requests. As described in section \ref{impl-backend-interfaces}, the \texttt{input} field in the request JSON determines which Fibonacci number is calculated by a worker component. This means the time a worker instance spends on executing a task can be manipulated by the input value.

\paragraph{Request frequency} Another method that the load generator can exploit to control the workload on the system is to wait different amounts of time between sending individual requests. This can also model how real users would use the application. The wait time between requests can be a constant value, but in fact, it can be calculated based upon any arbitrary logic. Generally, various kinds of distribution functions are used to determine the request frequency.

\paragraph{Parallelism} To increase the workload on the system, multiple requests can be sent by the load generator in parallel. Using this together with increasing the request frequency can greatly intensify the load on the application. Similarly to the request frequency, the amount of requests to send in parallel can be calculated using any logic.


%----------------------------------------------------------------------------
\subsection{Fault Injector} \label{test-design-fault-injector}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item inject faults on different levels (Pod, Network, CPU, Memory)
%	\item periodic fault injection
%\end{itemize}

The aim of the fault injector component is to introduce different kinds of failure anomalies to the application in order to trigger fault tolerant mechanisms in the system if they exist.

The amount of possible failure anomalies proportionally increases with the complexity of the system and is most likely to be close to practically countless variations. To achieve the best results while limiting the number of failure anomalies to test, only the most common failure scenarios should be taken into account. Considering also the probability of the failure anomalies and applying only those to the system can result in a fairly good approximation of real world failure scenarios.

The following sections attempt to collect some of the most common anomalies that can cause errors or even outages in cloud based systems.

%----------------------------------------------------------------------------
\subsubsection{General failure anomalies}
%----------------------------------------------------------------------------

\paragraph{Network anomalies} Network anomalies include latency, jitter, slow bandwith, network partition. All of these can affect the performance and dependability of an application.

\paragraph{CPU starvation} CPU starvation can occur if there are multiple software components deployed to the same physical machine which is the case most of the time when using the products of cloud provider companies to run an application. If a process on a machine executes a CPU heavy computation, it can reserve most of the CPU capacity of the machine, leaving other processes without enough processor time which can cause transient or even permanent application failures.

\paragraph{Memory starvation} Memory starvation is similar to CPU starvation, only instead of sharing a machine with CPU usage heavy processes, there are programs that take up most of the memory of the physical machine.

\paragraph{Disk anomalies} Disk anomalies are probably the most common failure anomalies as these type of devices are used in enormous amounts in every data center. This class of anomalies can frequently occur in several different forms, for example disk I/O latency, read/write errors or complete device failures.

%----------------------------------------------------------------------------
\subsubsection{Kubernetes failure anomalies}
%----------------------------------------------------------------------------

As the target platform is Kubernetes, the final list of applicable failure scenarios should include Kubernetes related anomalies as well. Some possible examples are listed below.

\paragraph{Pod failure} Pod failures can happen for example in case of configuration errors or application errors. Typically, when a pod fails, it gets restarted by a \texttt{ReplicaSet} that may solve transient errors.

\paragraph{Pod death} Pods can be terminated due to human operational faults but also when the Kubernetes control plane decides to reschedule a pod to a different physical machine.

\paragraph{Cloud provider anomalies} Although Kubernetes clusters can be set up on private machines as well, generally they are used as a service offered by a cloud provider company. These products are highly available and are managed by experts, however, occasional errors can possibly happen in these type of environments too.

%----------------------------------------------------------------------------
\subsection{Monitoring}
%----------------------------------------------------------------------------
%need to monitor the application metrics, state --> need for monitoring system
% which metrics to collect
% whicch dependability metrics to use
% timeseries metrics
% vizualisation capabilities

The framework should be able to collect metrics about the infrastructure and the sample application. This requires that the framework should have a separate monitoring subsystem.

As the value of different metrics changes over time while the system executes task and while various failure anomalies are injected, the monitoring system should be able to handle time series data format effectively.

In order to increase the human usability of the framework, it is required that the monitoring system provides a visual interface where its users can track the status of the system under test during execution and to evaluate results.

There are a lot of metrics that the monitoring system can collect. Apart from the application specific custom metric like \texttt{worker\_busy\_thread} introduced in section \ref{impl-worker-custom-metrics}, Kubernetes itself exposes a vast amount of metrics in several categories. As the focus of the thesis is on measuring the dependability metrics, only the values related to this should be collected and used. Section \ref{background-dep-metrics-mean-values} and \ref{background-dep-metrics-prob-funcs} describes the possible dependability metrics that can be measured. The monitoring system targets collecting measurements for the following dependability metrics:

\begin{itemize}
	\item Mean Up Time
	\item Mean Down Time
	\item Mean Time Between Failures
	\item Availability
\end{itemize}

These metrics were chosen because they acceptably describe the dependability characteristics of a cloud based system that is designed to function continuously. 

%----------------------------------------------------------------------------
\section{Implementation}
%----------------------------------------------------------------------------

%\begin{itemize}
%	\item Test environment locally or in cloud
%	\item how does running tests affect the 
%\end{itemize}

This section describes the implementation of the test framework and presents the decisions behind them.

Before going into the details of each subsystem in the framework, the environment of the test framework should be specified. The question is, where to run the framework? Should it be deployed next to the system under test to minimize communication overhead or should it run independently from the application?

Operating the test framework in the same environment as the tested application seems obvious at first, however this would bring in several issues that should be taken into account. Deploying the the framework and the system under the test would create a tightly coupled structure, whereas in distributed systems, loosely-coupled units are preferred. Co-deployment would also lead to the possibility that operating the framework could have a performance impact on the tested application itself, causing altered measurement results. Any infrastructural error (for example physical machine crash) -- either provoked by transient failures or inserted by the fault injector -- would affect both the application under test (which is accepted and should be handled by the application) and the framework itself. This is not passable as it would lead to framework malfunctions.

The conclusion is to deploy the test framework and the application to evaluate separately. As it is practical for some of the tools used in the framework to run in the same Kubernetes cluster as the application, separation is implemented with exploiting the namespacing capabilities of Kubernetes.

%----------------------------------------------------------------------------
\subsection{Kubedepend}
%----------------------------------------------------------------------------

Section \ref{test-design-req} emphasized the importance that the framework should be configurable, give reproducible results and be automated. To satisfy these requirements, a command line program in Python was created to enforce them as well as to implement the major part of the test framework workflow proposed in Section \ref{test-design-workflow}. This tool is named Kubedepend.

%----------------------------------------------------------------------------
\subsubsection{Configuration}
%----------------------------------------------------------------------------

Kubedepend supports easy configuration through exposing command line parameters that can be used to customize some behaviors of the tool.

This functionality is implemented with the help of the Click Python package \cite{Click}. Click enables creating clean command line interfaces in a composable way with as little code as necessary.

For instance, the user can specify the duration of the load generation, the types of failures anomalies to inject into the system and the number of tests to execute in sequence with the same configuration. The main configuration options are presented in later sections describing the implementation of the test framework workflow steps.

An example on how to define a command line parameter for the entrypoint of the Python program can be seen below.

\begin{minipage}{\linewidth}
	\begin{lstlisting}[language=python, caption={Define a parameter to control saving test results (simplified extract)}, label={lst:click-option}]
	@click.command()
	@click.option('--nosave', is_flag=True)
	def main(nosave):
		...
	
		click.echo('nosave=' + str(nosave))
		
		...
	\end{lstlisting}
\end{minipage}

%----------------------------------------------------------------------------
\subsubsection{Reproducibility}
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
\subsubsection{Automation}
%----------------------------------------------------------------------------

After providing it with the right inputs, Kubedepend facilitates automatic load generation, fault injection and metric collection with the help of other third party tools later discussed in this chapter. When it is started, it is assumed that the Kubernetes infrastructure and the application are already ready and deployed. This means that it practically manages all the steps in Figure \ref{fig:test_framework_workflow} except the first one -- Prepare infrastructure and application. This step is executed by another mechanism that is later discussed (see Section \ref{cicd}).

%\begin{itemize}
%	\item what it is used for?
%	\item facilitates automatic load generation, fault injection, collecting results
%	\item describe its workflow
%	\item purpose of waiting for stable system state
%	\item prometheus metrics
%	\item parameters (click)
%\end{itemize}

%----------------------------------------------------------------------------
\subsection{Load generation}
%----------------------------------------------------------------------------

\begin{itemize}
	\item locust
	\item usermodel
	\item parameterization
\end{itemize}

%----------------------------------------------------------------------------
\subsection{Fault Injection}
%----------------------------------------------------------------------------

\begin{itemize}
	\item deploy Chaos Mesh
	\item define chaos experiments according to fault profiles (show one as example)
	\item describe the usage of helm to deploy faults
\end{itemize}

%----------------------------------------------------------------------------
\subsubsection{Fault Profiles}
%----------------------------------------------------------------------------

\begin{itemize}
	\item list fault profiles and justification
\end{itemize}

%----------------------------------------------------------------------------
\subsection{Measurement} \label{measurement-impl}
%----------------------------------------------------------------------------

\begin{itemize}
	\item Deploy tools \begin{itemize}
		\item Prometheus
		\item Prometheus Adapter
		\item Promethues Blackbox Exporter
		\item Grafana
	\end{itemize}
	\item prometheus annotations, scrape config
	\item measurement sequence - what is the purpose of this
\end{itemize}

%----------------------------------------------------------------------------
\subsubsection{Metrics}
%----------------------------------------------------------------------------

\begin{itemize}
	
	\item dependability metrics in PromQL
	\item justify the 15 multiplier - it is the prometheus scrape interval
\end{itemize}

%----------------------------------------------------------------------------
\subsection{CI/CD Pipeline} \label{cicd}
%----------------------------------------------------------------------------

\begin{itemize}
	\item CF template
	\item Steps and what do they do
	\item image about jenkins pipeline steps
	\item parameters - passed to kubedepend
\end{itemize}